## 参考的blog相关链接

* [最小二乘估计(Least Squares Estimator)的公式的推导](http://www.qiujiawei.com/linear-algebra-15/)

* [梯度下降法的三种形式BGD、SGD以及MBGD](http://www.cnblogs.com/maybe2030/p/5089753.html)

* [逻辑回归模型(Logistic Regression, LR)基础](https://www.cnblogs.com/sparkwen/p/3441197.html)

* [深入浅出最大似然估计（Maximum Likelihood Estimation）](https://www.jianshu.com/p/f1d3906e4a3e)

* [极大似然估计详解](http://blog.csdn.net/zengxiantao1994/article/details/72787849)

* [logistic函数和softmax函数](http://www.cnblogs.com/maybe2030/p/5678387.html)

* [支持向量机系列](http://blog.pluskid.org/?page_id=683)

* [简易解说拉格朗日对偶（Lagrange duality）](http://www.cnblogs.com/90zeng/p/Lagrange_duality.html)

* [机器学习算法实践-SVM中的SMO算法](https://zhuanlan.zhihu.com/p/29212107)

* [SVM (支持向量机)](https://blog.csdn.net/endlch/article/details/46843903)

* [机器学习算法系列（18）：方差偏差权衡（Bias-Variance Tradeoff）](https://plushunter.github.io/2017/04/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8818%EF%BC%89%EF%BC%9A%E6%96%B9%E5%B7%AE%E5%81%8F%E5%B7%AE%E6%9D%83%E8%A1%A1%EF%BC%88Bias-Variance%20Tradeoff%EF%BC%89/)

* [机器学习中正则化项L1和L2的直观理解](https://www.jianshu.com/p/201d9917c578)

* [算法杂货铺——分类算法之决策树(Decision tree)](http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)

* [决策树之CART（分类回归树）详解](http://blog.csdn.net/zhihua_oba/article/details/72230427)

* [
机器学习算法系列（4）：决策树](https://plushunter.github.io/2017/01/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91/)

* [GBDT：梯度提升决策树](https://www.jianshu.com/p/005a4e6ac775)

* [XGBoost](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)

* [XGBoost 与 Boosted Tree](http://www.52cs.org/?p=429)

* [机器学习算法系列（6）：AdaBoost](https://plushunter.github.io/2017/01/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%886%EF%BC%89%EF%BC%9AAdaBoost/)

* [为什么说bagging是减少variance，而boosting是减少bias?
](https://www.zhihu.com/question/26760839/answer/40337791)

* [Free Will 小站](https://plushunter.github.io/tech-stack/)

* [ML From Scrath](https://github.com/HadXu/ML-From-Scratch)

* [AI](https://github.com/excelsimon/AI)

* [MachineLearning](https://github.com/wepe/MachineLearning)

* [几种常见的优化算法](https://www.cnblogs.com/jiahenhe2/archive/2017/12/21/8080563.html)

* [[Math] 常见的几种最优化方法](http://www.cnblogs.com/maybe2030/p/4751804.html)

* [理解梯度下降](http://liuchengxu.org/blog-cn/posts/dive-into-gradient-decent/)

* [牛顿法和梯度下降法基础](https://www.jianshu.com/p/e976e1853cb9)

* [算法细节系列（3）：梯度下降法，牛顿法，拟牛顿法
](https://blog.csdn.net/u014688145/article/details/53688585
)

* [Newton's method and Quasi Newton method牛顿法与拟牛顿法
](http://littlehaes.com/2018/03/26/Newton-method-and-quasi-Newton-method%E7%89%9B%E9%A1%BF%E6%B3%95%E4%B8%8E%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/
)

* [优化算法——拟牛顿法之L-BFGS算法
](http://www.itboth.com/d/YJV36f/l-bfgs-bfgs)

* [优化算法——拟牛顿法之BFGS算法
](http://www.itboth.com/d/7J77va/bfgs)

* [浅析FTRL-PROXIMAL算法
](https://zhuanlan.zhihu.com/p/35449814
)

* [FTRL-大规模LR模型
](http://www.datakit.cn/blog/2016/05/11/ftrl.html
)

* [零基础入门深度学习(4) - 卷积神经网络
](https://www.zybuluo.com/hanbingtao/note/485480
)

* [详解机器学习中的梯度消失、爆炸原因及其解决方法
](https://blog.csdn.net/qq_25737169/article/details/78847691)

* [《深度学习》课堂笔记](http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Neural_Networks_and_Deep_Learning/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80)












